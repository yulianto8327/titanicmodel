{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2307f45",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-31T09:01:25.344013Z",
     "iopub.status.busy": "2024-12-31T09:01:25.343614Z",
     "iopub.status.idle": "2024-12-31T09:01:28.903386Z",
     "shell.execute_reply": "2024-12-31T09:01:28.901820Z"
    },
    "papermill": {
     "duration": 3.567737,
     "end_time": "2024-12-31T09:01:28.906244",
     "exception": false,
     "start_time": "2024-12-31T09:01:25.338507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xgboost as xgb\n",
    "\n",
    "# Function to calculate correlation after imputation\n",
    "def evaluate_imputation(data, column, target, strategy, constant_value=None):\n",
    "    \"\"\"\n",
    "    Evaluate the correlation between a column and the target column after imputation.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing the column and target.\n",
    "        column (str): The name of the column to impute.\n",
    "        target (str): The name of the target column.\n",
    "        strategy (str): The imputation strategy ('mean', 'constant').\n",
    "        constant_value: The value to use for constant imputation (only needed if strategy is 'constant').\n",
    "\n",
    "    Returns:\n",
    "        float: Correlation between the imputed column and the target column.\n",
    "    \"\"\"\n",
    "    imputer = None\n",
    "    if strategy == 'constant':\n",
    "        if constant_value is None:\n",
    "            raise ValueError(\"Constant value must be provided for 'constant' strategy.\")\n",
    "        imputer = SimpleImputer(strategy='constant', fill_value=constant_value)\n",
    "    elif strategy == 'mean':\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported strategy: {strategy}\")\n",
    "\n",
    "    # Impute the missing values\n",
    "    imputed_column = imputer.fit_transform(data[[column]])\n",
    "    \n",
    "    # Replace original column with imputed values for correlation calculation\n",
    "    data_imputed = data.copy()\n",
    "    data_imputed[column] = imputed_column\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = data_imputed[column].corr(data_imputed[target])\n",
    "    return correlation\n",
    "    \n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the dataset by handling missing values, infinite values, and preparing it for analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pd.DataFrame - The dataset to clean.\n",
    "    \n",
    "    Returns:\n",
    "    - cleaned_df: pd.DataFrame - The cleaned dataset.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print(\"\\nCleaning Data...\")\n",
    "    # Replace infinities with NaN\n",
    "    df.replace([float('inf'), -float('inf')], float('nan'), inplace=True)\n",
    "    \n",
    "    # Report missing values before cleaning\n",
    "    missing_before = df.isnull().sum().sum()\n",
    "    print(f\"Missing values before cleaning: {missing_before}\")\n",
    "    \n",
    "    # Fill NaN values with column means for numerical features\n",
    "    df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "    \n",
    "    # Report missing values after cleaning\n",
    "    missing_after = df.isnull().sum().sum()\n",
    "    print(f\"Missing values after cleaning: {missing_after}\")\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def analyze_data(df, target_column=None, correlation_columns=None, plot_columns=None, calculate_correlations=False, calculate_mi=False):\n",
    "    \"\"\"\n",
    "    Analyze data by calculating statistics, correlations, unique values, empty values, and creating visualizations.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pd.DataFrame - The dataset to analyze.\n",
    "    - target_column: str - The target column for correlation analysis.\n",
    "    - correlation_columns: list - List of columns to calculate correlations with the target column.\n",
    "    - plot_columns: list - List of columns to visualize trends or distributions.\n",
    "    \n",
    "    Returns:\n",
    "    - stats: pd.DataFrame - Summary statistics of the dataset.\n",
    "    - correlations: pd.Series or None - Correlations with the target column, if specified.\n",
    "    - unique_values: pd.Series - Count of unique values per column.\n",
    "    - empty_values: pd.Series - Count of empty (NaN) values per column.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nShape:\")\n",
    "    print(df.shape)\n",
    "    \n",
    "    print(\"\\nInfo:\")\n",
    "    df.info()\n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    df_encoded = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "\n",
    "    # 1. Display descriptive statistics\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    stats = df.describe(include='all')\n",
    "    #with pd.option_context('display.max_rows', None):  # Show all rows\n",
    "    #    print(stats)\n",
    "    \n",
    "    # 2. Check unique values\n",
    "    print(\"\\nUnique Object Values per Column > 100:\")\n",
    "    unique_values = df.nunique()\n",
    "    #with pd.option_context('display.max_rows', None):  # Show all rows\n",
    "    #    print(unique_values)\n",
    "\n",
    "    result = {col: df[col].nunique() for col in df.select_dtypes(include='object') if df[col].nunique() > 100}\n",
    "\n",
    "    print(result)\n",
    "    \n",
    "    # 3. Check empty (NaN) values\n",
    "    print(\"\\nEmpty (NaN) Values per Column > 50%:\")\n",
    "    empty_values = df.isnull().sum()\n",
    "    #with pd.option_context('display.max_rows', None):  # Show all rows\n",
    "    #    print(empty_values)\n",
    "    threshold = 0.5\n",
    "    empty_columns = df.columns[df.isnull().mean() > threshold]\n",
    "    \n",
    "    # Display the result\n",
    "    print(empty_columns.tolist())\n",
    "\n",
    "    # 4. Calculate correlations\n",
    "    correlations = None\n",
    "    if calculate_correlations and target_column:\n",
    "        if correlation_columns is None:\n",
    "            correlation_columns = [col for col in df.columns if col != target_column]\n",
    "        print(f\"\\nCorrelations with '{target_column}':\")\n",
    "        correlations = df[correlation_columns].corrwith(df[target_column]).abs().sort_values(ascending=False)\n",
    "        print(correlations)\n",
    "\n",
    "    # 4. Calculate simple correlations\n",
    "    corr_matrix = df_encoded.corr()\n",
    "    print(\"\\High correlations\")\n",
    "    # Highlight correlations above a threshold\n",
    "    threshold = 0.9  # Define multicollinearity threshold\n",
    "    high_corr = corr_matrix[(corr_matrix > threshold) & (corr_matrix != 1.0)]\n",
    "        \n",
    "    print(\"\\nHighly Correlated Pairs (Threshold > \"+ str(threshold) +\"):\")\n",
    "    for col in high_corr.columns:\n",
    "        for row in high_corr.index:\n",
    "            if pd.notnull(high_corr.loc[row, col]):\n",
    "                print(f\"{row} and {col} have correlation: {corr_matrix.loc[row, col]}\")\n",
    "\n",
    "    # 5. Visualize data\n",
    "    if plot_columns:\n",
    "        print(\"\\nVisualizing data:\")\n",
    "        for column in plot_columns:\n",
    "            if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                # Histogram for numerical data\n",
    "                sns.histplot(df[column], kde=True)\n",
    "                plt.title(f\"Distribution of {column}\")\n",
    "                plt.show()\n",
    "            else:\n",
    "                # Bar plot for categorical data\n",
    "                sns.countplot(data=df, x=column)\n",
    "                plt.title(f\"Countplot of {column}\")\n",
    "                plt.show()\n",
    "    \n",
    "    # 6. Calculate MI scores\n",
    "    mi_scores = None\n",
    "    if calculate_mi and target_column:\n",
    "        print(\"\\nMutual Information (MI) Scores:\")\n",
    "        X = df_encoded.drop(columns=[target_column])\n",
    "        y = df_encoded[target_column]\n",
    "        mi_scores_array = mutual_info_regression(X, y, random_state=42)\n",
    "        mi_scores = pd.Series(mi_scores_array, index=X.columns).sort_values(ascending=False)\n",
    "        \n",
    "        with pd.option_context('display.max_rows', None):  # Show all rows\n",
    "            print(mi_scores)\n",
    "     \n",
    "    return stats, correlations, unique_values, empty_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcbd6e3",
   "metadata": {
    "papermill": {
     "duration": 0.00444,
     "end_time": "2024-12-31T09:01:28.915409",
     "exception": false,
     "start_time": "2024-12-31T09:01:28.910969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Load and check the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26981fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T09:01:28.925145Z",
     "iopub.status.busy": "2024-12-31T09:01:28.924282Z",
     "iopub.status.idle": "2024-12-31T09:01:30.111959Z",
     "shell.execute_reply": "2024-12-31T09:01:30.110647Z"
    },
    "papermill": {
     "duration": 1.195477,
     "end_time": "2024-12-31T09:01:30.114099",
     "exception": false,
     "start_time": "2024-12-31T09:01:28.918622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start DATA\n",
      "\n",
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... MiscFeature MiscVal MoSold YrSold SaleType  \\\n",
      "0         Lvl    AllPub  ...         NaN       0      2   2008       WD   \n",
      "1         Lvl    AllPub  ...         NaN       0      5   2007       WD   \n",
      "2         Lvl    AllPub  ...         NaN       0      9   2008       WD   \n",
      "3         Lvl    AllPub  ...         NaN       0      2   2006       WD   \n",
      "4         Lvl    AllPub  ...         NaN       0     12   2008       WD   \n",
      "\n",
      "  SaleCondition SalePrice  BsmtBath  AboveGradeBath  TotalPorchArea  \n",
      "0        Normal    208500         1               3              61  \n",
      "1        Normal    181500         1               2             298  \n",
      "2        Normal    223500         1               3              42  \n",
      "3       Abnorml    140000         1               1             307  \n",
      "4        Normal    250000         1               3             276  \n",
      "\n",
      "[5 rows x 84 columns]\n",
      "\n",
      "Shape:\n",
      "(1460, 84)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 84 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Id              1460 non-null   int64  \n",
      " 1   MSSubClass      1460 non-null   int64  \n",
      " 2   MSZoning        1460 non-null   object \n",
      " 3   LotFrontage     1201 non-null   float64\n",
      " 4   LotArea         1460 non-null   int64  \n",
      " 5   Street          1460 non-null   object \n",
      " 6   Alley           91 non-null     object \n",
      " 7   LotShape        1460 non-null   object \n",
      " 8   LandContour     1460 non-null   object \n",
      " 9   Utilities       1460 non-null   object \n",
      " 10  LotConfig       1460 non-null   object \n",
      " 11  LandSlope       1460 non-null   object \n",
      " 12  Neighborhood    1460 non-null   object \n",
      " 13  Condition1      1460 non-null   object \n",
      " 14  Condition2      1460 non-null   object \n",
      " 15  BldgType        1460 non-null   object \n",
      " 16  HouseStyle      1460 non-null   object \n",
      " 17  OverallQual     1460 non-null   int64  \n",
      " 18  OverallCond     1460 non-null   int64  \n",
      " 19  YearBuilt       1460 non-null   int64  \n",
      " 20  YearRemodAdd    1460 non-null   int64  \n",
      " 21  RoofStyle       1460 non-null   object \n",
      " 22  RoofMatl        1460 non-null   object \n",
      " 23  Exterior1st     1460 non-null   object \n",
      " 24  Exterior2nd     1460 non-null   object \n",
      " 25  MasVnrType      588 non-null    object \n",
      " 26  MasVnrArea      1452 non-null   float64\n",
      " 27  ExterQual       1460 non-null   int64  \n",
      " 28  ExterCond       1460 non-null   int64  \n",
      " 29  Foundation      1460 non-null   object \n",
      " 30  BsmtQual        1423 non-null   float64\n",
      " 31  BsmtCond        1423 non-null   float64\n",
      " 32  BsmtExposure    1422 non-null   object \n",
      " 33  BsmtFinType1    1423 non-null   object \n",
      " 34  BsmtFinSF1      1460 non-null   int64  \n",
      " 35  BsmtFinType2    1422 non-null   object \n",
      " 36  BsmtFinSF2      1460 non-null   int64  \n",
      " 37  BsmtUnfSF       1460 non-null   int64  \n",
      " 38  TotalBsmtSF     1460 non-null   int64  \n",
      " 39  Heating         1460 non-null   object \n",
      " 40  HeatingQC       1460 non-null   int64  \n",
      " 41  CentralAir      1460 non-null   object \n",
      " 42  Electrical      1459 non-null   object \n",
      " 43  1stFlrSF        1460 non-null   int64  \n",
      " 44  2ndFlrSF        1460 non-null   int64  \n",
      " 45  LowQualFinSF    1460 non-null   int64  \n",
      " 46  GrLivArea       1460 non-null   int64  \n",
      " 47  BsmtFullBath    1460 non-null   int64  \n",
      " 48  BsmtHalfBath    1460 non-null   int64  \n",
      " 49  FullBath        1460 non-null   int64  \n",
      " 50  HalfBath        1460 non-null   int64  \n",
      " 51  BedroomAbvGr    1460 non-null   int64  \n",
      " 52  KitchenAbvGr    1460 non-null   int64  \n",
      " 53  KitchenQual     1460 non-null   int64  \n",
      " 54  TotRmsAbvGrd    1460 non-null   int64  \n",
      " 55  Functional      1460 non-null   object \n",
      " 56  Fireplaces      1460 non-null   int64  \n",
      " 57  FireplaceQu     770 non-null    float64\n",
      " 58  GarageType      1379 non-null   object \n",
      " 59  GarageYrBlt     1379 non-null   float64\n",
      " 60  GarageFinish    1379 non-null   object \n",
      " 61  GarageCars      1460 non-null   int64  \n",
      " 62  GarageArea      1460 non-null   int64  \n",
      " 63  GarageQual      1379 non-null   float64\n",
      " 64  GarageCond      1379 non-null   float64\n",
      " 65  PavedDrive      1460 non-null   object \n",
      " 66  WoodDeckSF      1460 non-null   int64  \n",
      " 67  OpenPorchSF     1460 non-null   int64  \n",
      " 68  EnclosedPorch   1460 non-null   int64  \n",
      " 69  3SsnPorch       1460 non-null   int64  \n",
      " 70  ScreenPorch     1460 non-null   int64  \n",
      " 71  PoolArea        1460 non-null   int64  \n",
      " 72  PoolQC          7 non-null      float64\n",
      " 73  Fence           281 non-null    object \n",
      " 74  MiscFeature     54 non-null     object \n",
      " 75  MiscVal         1460 non-null   int64  \n",
      " 76  MoSold          1460 non-null   int64  \n",
      " 77  YrSold          1460 non-null   int64  \n",
      " 78  SaleType        1460 non-null   object \n",
      " 79  SaleCondition   1460 non-null   object \n",
      " 80  SalePrice       1460 non-null   int64  \n",
      " 81  BsmtBath        1460 non-null   int64  \n",
      " 82  AboveGradeBath  1460 non-null   int64  \n",
      " 83  TotalPorchArea  1460 non-null   int64  \n",
      "dtypes: float64(9), int64(42), object(33)\n",
      "memory usage: 958.2+ KB\n",
      "\n",
      "Descriptive Statistics:\n",
      "\n",
      "Unique Object Values per Column > 100:\n",
      "{}\n",
      "\n",
      "Empty (NaN) Values per Column > 50%:\n",
      "['Alley', 'MasVnrType', 'PoolQC', 'Fence', 'MiscFeature']\n",
      "\\High correlations\n",
      "\n",
      "Highly Correlated Pairs (Threshold > 0.9):\n",
      "Exterior2nd_CmentBd and Exterior1st_CemntBd have correlation: 0.9741710841939724\n",
      "Exterior2nd_MetalSd and Exterior1st_MetalSd have correlation: 0.9730651937625319\n",
      "Exterior2nd_VinylSd and Exterior1st_VinylSd have correlation: 0.9775248873036357\n",
      "Exterior1st_CemntBd and Exterior2nd_CmentBd have correlation: 0.9741710841939724\n",
      "Exterior1st_MetalSd and Exterior2nd_MetalSd have correlation: 0.9730651937625319\n",
      "Exterior1st_VinylSd and Exterior2nd_VinylSd have correlation: 0.9775248873036357\n",
      "SaleCondition_Partial and SaleType_New have correlation: 0.9868189596845142\n",
      "SaleType_New and SaleCondition_Partial have correlation: 0.9868189596845142\n",
      "\n",
      "correlation between OverallQual and OverallCond\n",
      "             OverallQual  OverallCond\n",
      "OverallQual     1.000000    -0.091932\n",
      "OverallCond    -0.091932     1.000000\n",
      "\n",
      "correlation between YearBuilt and YearRemodAdd\n",
      "              YearBuilt  YearRemodAdd\n",
      "YearBuilt      1.000000      0.592855\n",
      "YearRemodAdd   0.592855      1.000000\n",
      "\n",
      "correlation between GarageCars and GarageArea\n",
      "            GarageCars  GarageArea\n",
      "GarageCars    1.000000    0.882475\n",
      "GarageArea    0.882475    1.000000\n"
     ]
    }
   ],
   "source": [
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "train_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
    "train_data.head()\n",
    "test_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
    "test_data.head()\n",
    "\n",
    "#mapping numeric ordinal\n",
    "mapping = {\n",
    "    'Ex': 5,\n",
    "    'Gd': 4,\n",
    "    'TA': 3,\n",
    "    'Fa': 2,\n",
    "    'Po': 1,\n",
    "    'NA': 0  # or NaN if you prefer\n",
    "}\n",
    "#ExterQual\n",
    "#ExterCond\n",
    "#BsmtQual\n",
    "#BsmtCond\n",
    "#HeatingQC\n",
    "#KitchenQual\n",
    "#FireplaceQu\n",
    "#GarageQual\n",
    "#GarageCond\n",
    "#PoolQC\n",
    "\n",
    "train_data[\"BsmtBath\"] = train_data[\"BsmtFullBath\"] + train_data[\"BsmtHalfBath\"]\n",
    "train_data[\"AboveGradeBath\"] = train_data[\"FullBath\"] + train_data[\"HalfBath\"]\n",
    "train_data[\"TotalPorchArea\"] = (train_data[\"WoodDeckSF\"] + \n",
    "                                 train_data[\"OpenPorchSF\"] + \n",
    "                                 train_data[\"EnclosedPorch\"] + \n",
    "                                 train_data[\"3SsnPorch\"] + \n",
    "                                 train_data[\"ScreenPorch\"])\n",
    "train_data['ExterQual'] = train_data['ExterQual'].map(mapping)\n",
    "train_data['ExterCond'] = train_data['ExterCond'].map(mapping)\n",
    "train_data['BsmtQual'] = train_data['BsmtQual'].map(mapping)\n",
    "train_data['BsmtCond'] = train_data['BsmtCond'].map(mapping)\n",
    "train_data['HeatingQC'] = train_data['HeatingQC'].map(mapping)\n",
    "train_data['KitchenQual'] = train_data['KitchenQual'].map(mapping)\n",
    "train_data['FireplaceQu'] = train_data['FireplaceQu'].map(mapping)\n",
    "train_data['GarageQual'] = train_data['GarageQual'].map(mapping)\n",
    "train_data['GarageCond'] = train_data['GarageCond'].map(mapping)\n",
    "train_data['PoolQC'] = train_data['PoolQC'].map(mapping)\n",
    "\n",
    "test_data[\"BsmtBath\"] = test_data[\"BsmtFullBath\"] + test_data[\"BsmtHalfBath\"]\n",
    "test_data[\"AboveGradeBath\"] = test_data[\"FullBath\"] + test_data[\"HalfBath\"]\n",
    "test_data[\"TotalPorchArea\"] = (test_data[\"WoodDeckSF\"] + \n",
    "                                 test_data[\"OpenPorchSF\"] + \n",
    "                                 test_data[\"EnclosedPorch\"] + \n",
    "                                 test_data[\"3SsnPorch\"] + \n",
    "                                 test_data[\"ScreenPorch\"])\n",
    "test_data['ExterQual'] = test_data['ExterQual'].map(mapping)\n",
    "test_data['ExterCond'] = test_data['ExterCond'].map(mapping)\n",
    "test_data['BsmtQual'] = test_data['BsmtQual'].map(mapping)\n",
    "test_data['BsmtCond'] = test_data['BsmtCond'].map(mapping)\n",
    "test_data['HeatingQC'] = test_data['HeatingQC'].map(mapping)\n",
    "test_data['KitchenQual'] = test_data['KitchenQual'].map(mapping)\n",
    "test_data['FireplaceQu'] = test_data['FireplaceQu'].map(mapping)\n",
    "test_data['GarageQual'] = test_data['GarageQual'].map(mapping)\n",
    "test_data['GarageCond'] = test_data['GarageCond'].map(mapping)\n",
    "test_data['PoolQC'] = test_data['PoolQC'].map(mapping)\n",
    "\n",
    "print(\"\\nStart DATA\\n\")\n",
    "print(train_data.head())\n",
    "\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "stats, correlations, unique_values, empty_values = analyze_data(\n",
    "    df,\n",
    "    target_column=\"SalePrice\",  # Example target column\n",
    "    calculate_mi = False\n",
    ")\n",
    "\n",
    "# Check the correlation between OverallQual and OverallCond\n",
    "print(\"\\ncorrelation between OverallQual and OverallCond\")\n",
    "print(train_data[[\"OverallQual\", \"OverallCond\"]].corr())\n",
    "\n",
    "# Check the correlation between OverallQual and OverallCond\n",
    "print(\"\\ncorrelation between YearBuilt and YearRemodAdd\")\n",
    "print(train_data[[\"YearBuilt\", \"YearRemodAdd\"]].corr())\n",
    "\n",
    "# Check the correlation between GarageCars and GarageArea\n",
    "print(\"\\ncorrelation between GarageCars and GarageArea\")\n",
    "print(train_data[[\"GarageCars\", \"GarageArea\"]].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7694e8cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T09:01:30.123551Z",
     "iopub.status.busy": "2024-12-31T09:01:30.123101Z",
     "iopub.status.idle": "2024-12-31T09:01:30.157660Z",
     "shell.execute_reply": "2024-12-31T09:01:30.156339Z"
    },
    "papermill": {
     "duration": 0.041678,
     "end_time": "2024-12-31T09:01:30.159664",
     "exception": false,
     "start_time": "2024-12-31T09:01:30.117986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL DATA\n",
      "\n",
      "   MSSubClass MSZoning  LotFrontage  LotArea LotShape LandContour Utilities  \\\n",
      "0          60       RL         65.0     8450      Reg         Lvl    AllPub   \n",
      "1          20       RL         80.0     9600      Reg         Lvl    AllPub   \n",
      "2          60       RL         68.0    11250      IR1         Lvl    AllPub   \n",
      "3          70       RL         60.0     9550      IR1         Lvl    AllPub   \n",
      "4          60       RL         84.0    14260      IR1         Lvl    AllPub   \n",
      "\n",
      "  LotConfig Neighborhood Condition1 BldgType HouseStyle  OverallQual  \\\n",
      "0    Inside      CollgCr       Norm     1Fam     2Story            7   \n",
      "1       FR2      Veenker      Feedr     1Fam     1Story            6   \n",
      "2    Inside      CollgCr       Norm     1Fam     2Story            7   \n",
      "3    Corner      Crawfor       Norm     1Fam     2Story            7   \n",
      "4       FR2      NoRidge       Norm     1Fam     2Story            8   \n",
      "\n",
      "   OverallCond  YearBuilt  YearRemodAdd RoofStyle Exterior1st  MasVnrArea  \\\n",
      "0            5       2003          2003     Gable     VinylSd       196.0   \n",
      "1            8       1976          1976     Gable     MetalSd         0.0   \n",
      "2            5       2001          2002     Gable     VinylSd       162.0   \n",
      "3            5       1915          1970     Gable     Wd Sdng         0.0   \n",
      "4            5       2000          2000     Gable     VinylSd       350.0   \n",
      "\n",
      "   ExterQual  ExterCond Foundation  BsmtQual  BsmtCond BsmtExposure  \\\n",
      "0          4          3      PConc       4.0       3.0           No   \n",
      "1          3          3     CBlock       4.0       3.0           Gd   \n",
      "2          4          3      PConc       4.0       3.0           Mn   \n",
      "3          3          3     BrkTil       3.0       4.0           No   \n",
      "4          4          3      PConc       4.0       3.0           Av   \n",
      "\n",
      "  BsmtFinType1 BsmtFinType2  TotalBsmtSF Heating  HeatingQC CentralAir  \\\n",
      "0          GLQ          Unf        856.0    GasA          5          Y   \n",
      "1          ALQ          Unf       1262.0    GasA          5          Y   \n",
      "2          GLQ          Unf        920.0    GasA          5          Y   \n",
      "3          ALQ          Unf        756.0    GasA          4          Y   \n",
      "4          GLQ          Unf       1145.0    GasA          5          Y   \n",
      "\n",
      "  Electrical  GrLivArea  BedroomAbvGr  KitchenAbvGr  KitchenQual  \\\n",
      "0      SBrkr       1710             3             1          4.0   \n",
      "1      SBrkr       1262             3             1          3.0   \n",
      "2      SBrkr       1786             3             1          4.0   \n",
      "3      SBrkr       1717             3             1          4.0   \n",
      "4      SBrkr       2198             4             1          4.0   \n",
      "\n",
      "   TotRmsAbvGrd Functional  Fireplaces  FireplaceQu GarageType  GarageYrBlt  \\\n",
      "0             8        Typ           0          NaN     Attchd       2003.0   \n",
      "1             6        Typ           1          3.0     Attchd       1976.0   \n",
      "2             6        Typ           1          3.0     Attchd       2001.0   \n",
      "3             7        Typ           1          4.0     Detchd       1998.0   \n",
      "4             9        Typ           1          3.0     Attchd       2000.0   \n",
      "\n",
      "  GarageFinish  GarageArea  GarageQual  GarageCond PavedDrive  PoolArea  \\\n",
      "0          RFn       548.0         3.0         3.0          Y         0   \n",
      "1          RFn       460.0         3.0         3.0          Y         0   \n",
      "2          RFn       608.0         3.0         3.0          Y         0   \n",
      "3          Unf       642.0         3.0         3.0          Y         0   \n",
      "4          RFn       836.0         3.0         3.0          Y         0   \n",
      "\n",
      "   MiscVal  SalePrice  BsmtBath  AboveGradeBath  TotalPorchArea  \n",
      "0        0   208500.0       1.0               3              61  \n",
      "1        0   181500.0       1.0               2             298  \n",
      "2        0   223500.0       1.0               3              42  \n",
      "3        0   140000.0       1.0               1             307  \n",
      "4        0   250000.0       1.0               3             276  \n"
     ]
    }
   ],
   "source": [
    "td = pd.concat([train_data, test_data], ignore_index=True, sort  = False)\n",
    "\n",
    "#column with low correlation\n",
    "columns_to_drop = [\"Id\",\"MoSold\", \"YrSold\", \"MiscFeature\", \"RoofMatl\", \"LowQualFinSF\", \"Condition2\", \"LandSlope\",\"Street\"]\n",
    "\n",
    "#column with high correlation, and we feel it's not correlated with price\n",
    "columns_to_drop.extend([\"SaleType\", \"SaleCondition\", \"Exterior2nd\"])\n",
    "\n",
    "#BsmtFinSF1\tBsmtFinSF2 BsmtUnfSF =\tTotalBsmtSF\n",
    "columns_to_drop.extend([\"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\"])\n",
    "\n",
    "##1stFlrSF + 2ndFlrSF = GrLivArea\n",
    "columns_to_drop.extend([\"1stFlrSF\", \"2ndFlrSF\"])\n",
    "\n",
    "#optimize batthroom\n",
    "##BsmtFullBath: Basement full bathrooms\n",
    "#BsmtHalfBath: Basement half bathrooms\n",
    "#FullBath: Full bathrooms above grade\n",
    "#HalfBath: Half baths above grade\n",
    "#replaced with BsmtBath and AboveGradeBath\n",
    "columns_to_drop.extend([\"BsmtFullBath\", \"BsmtHalfBath\", \"FullBath\", \"HalfBath\"])\n",
    "\n",
    "#GarageCars and GarageArea, correlated, so we just remove GarageCars\n",
    "columns_to_drop.extend([\"GarageCars\"])\n",
    "\n",
    "#we combine the following to TotalPorchArea\n",
    "columns_to_drop.extend([\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\"])\n",
    "\n",
    "#Dropped columns due to high missing values:\n",
    "columns_to_drop.extend([\"Alley\",\"MasVnrType\",\"PoolQC\",\"Fence\",\"MiscFeature\"])\n",
    "\n",
    "td = td.drop(columns=columns_to_drop)\n",
    "\n",
    "#OneHotEncoder\n",
    "#td = pd.get_dummies(td, drop_first = True)\n",
    "\n",
    "print(\"\\nFINAL DATA\\n\")\n",
    "print(td.head())\n",
    "#td still have SalePrice\n",
    "\n",
    "X_test = td[td.SalePrice.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3312715c",
   "metadata": {
    "papermill": {
     "duration": 0.00332,
     "end_time": "2024-12-31T09:01:30.166710",
     "exception": false,
     "start_time": "2024-12-31T09:01:30.163390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. Drop not important column\n",
    "2. update columns\n",
    "   1. Preprocessing for numerical data, if empty, then numberic column change to median value\n",
    "   2. for string, if it's empty then set to most frequently use\n",
    "   3. Apply one hot \n",
    "\n",
    "proprocessor manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab7f6aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T09:01:30.175338Z",
     "iopub.status.busy": "2024-12-31T09:01:30.174958Z",
     "iopub.status.idle": "2024-12-31T09:01:30.291481Z",
     "shell.execute_reply": "2024-12-31T09:01:30.289626Z"
    },
    "papermill": {
     "duration": 0.1232,
     "end_time": "2024-12-31T09:01:30.293526",
     "exception": false,
     "start_time": "2024-12-31T09:01:30.170326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data\n",
      "(1168, 53)\n",
      "(292, 53)\n",
      "(1168, 1)\n",
      "(292, 1)\n",
      "After processing Data\n",
      "(1168, 132)\n",
      "(292, 132)\n",
      "(1459, 132)\n",
      "[[2.0000e+01 8.0000e+01 1.1622e+04 ... 0.0000e+00 0.0000e+00 1.0000e+00]\n",
      " [2.0000e+01 8.1000e+01 1.4267e+04 ... 0.0000e+00 0.0000e+00 1.0000e+00]\n",
      " [6.0000e+01 7.4000e+01 1.3830e+04 ... 0.0000e+00 0.0000e+00 1.0000e+00]\n",
      " ...\n",
      " [2.0000e+01 1.6000e+02 2.0000e+04 ... 0.0000e+00 0.0000e+00 1.0000e+00]\n",
      " [8.5000e+01 6.2000e+01 1.0441e+04 ... 0.0000e+00 0.0000e+00 1.0000e+00]\n",
      " [6.0000e+01 7.4000e+01 9.6270e+03 ... 0.0000e+00 0.0000e+00 1.0000e+00]]\n",
      "(1460, 1)\n",
      "   SalePrice\n",
      "0   208500.0\n",
      "1   181500.0\n",
      "2   223500.0\n",
      "3   140000.0\n",
      "4   250000.0\n"
     ]
    }
   ],
   "source": [
    "X = td[td.SalePrice.notnull()]\n",
    "y = X[[ 'SalePrice']].copy()\n",
    "y_aligned = y.iloc[1:]  # Removes the first row\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Original Data\")\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X.columns if\n",
    "                    X[cname].nunique() < 10 and \n",
    "                    X[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X.columns if \n",
    "                X[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "special_na_zero_cols = [\"MasVnrArea\"]\n",
    "\n",
    "#remove special_na_zero_cols from numerical_cols\n",
    "numerical_cols = [col for col in numerical_cols if col not in special_na_zero_cols]\n",
    "\n",
    "# Preprocessing for numerical data, if empty, then numberic column change to median value, median should be better\n",
    "numerical_transformer = SimpleImputer(strategy='median')\n",
    "numeric_na_zero_transformer = SimpleImputer(strategy='constant', fill_value=0)  # For the special column\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "#imputer change null to numeric\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('special', numeric_na_zero_transformer, special_na_zero_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing manually\n",
    "preprocessor.fit(X_train)  # Fit the preprocessor on training data\n",
    "X_train_preprocessed = preprocessor.transform(X_train)  # Transform training data\n",
    "X_val_preprocessed = preprocessor.transform(X_val)      # Transform validation data\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"After processing Data\")\n",
    "print(X_train_preprocessed.shape)\n",
    "print(X_val_preprocessed.shape)\n",
    "print(X_test_preprocessed.shape)\n",
    "print(X_test_preprocessed)\n",
    "print(y.shape)\n",
    "print(y.head())\n",
    "\n",
    "# Convert transformed data to DataFrame if needed (optional)\n",
    "# This step is useful if you want feature names for debugging.\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f60f71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T09:01:30.302908Z",
     "iopub.status.busy": "2024-12-31T09:01:30.302377Z",
     "iopub.status.idle": "2024-12-31T09:10:16.670266Z",
     "shell.execute_reply": "2024-12-31T09:10:16.669386Z"
    },
    "papermill": {
     "duration": 526.394934,
     "end_time": "2024-12-31T09:10:16.692526",
     "exception": false,
     "start_time": "2024-12-31T09:01:30.297592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:79790.65528\n",
      "[1]\tvalidation_0-rmse:72720.35957\n",
      "[2]\tvalidation_0-rmse:66318.70484\n",
      "[3]\tvalidation_0-rmse:60516.99140\n",
      "[4]\tvalidation_0-rmse:55215.24514\n",
      "[5]\tvalidation_0-rmse:50450.29646\n",
      "[6]\tvalidation_0-rmse:46284.72753\n",
      "[7]\tvalidation_0-rmse:42461.57361\n",
      "[8]\tvalidation_0-rmse:39011.55227\n",
      "[9]\tvalidation_0-rmse:35900.87992\n",
      "[10]\tvalidation_0-rmse:33098.81831\n",
      "[11]\tvalidation_0-rmse:30592.62606\n",
      "[12]\tvalidation_0-rmse:28533.46687\n",
      "[13]\tvalidation_0-rmse:26717.97857\n",
      "[14]\tvalidation_0-rmse:25126.47146\n",
      "[15]\tvalidation_0-rmse:23738.38403\n",
      "[16]\tvalidation_0-rmse:22527.82322\n",
      "[17]\tvalidation_0-rmse:21485.73188\n",
      "[18]\tvalidation_0-rmse:20556.49196\n",
      "[19]\tvalidation_0-rmse:19785.75407\n",
      "[20]\tvalidation_0-rmse:19204.14049\n",
      "[21]\tvalidation_0-rmse:18744.12579\n",
      "[22]\tvalidation_0-rmse:18207.65017\n",
      "[23]\tvalidation_0-rmse:17420.55377\n",
      "[24]\tvalidation_0-rmse:17002.75596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25]\tvalidation_0-rmse:16369.06932\n",
      "[26]\tvalidation_0-rmse:15793.20341\n",
      "[27]\tvalidation_0-rmse:15257.55372\n",
      "[28]\tvalidation_0-rmse:14764.35644\n",
      "[29]\tvalidation_0-rmse:14332.82083\n",
      "[30]\tvalidation_0-rmse:13907.63344\n",
      "[31]\tvalidation_0-rmse:13526.37816\n",
      "[32]\tvalidation_0-rmse:13154.94663\n",
      "[33]\tvalidation_0-rmse:12831.93681\n",
      "[34]\tvalidation_0-rmse:12767.80652\n",
      "[35]\tvalidation_0-rmse:12498.66725\n",
      "[36]\tvalidation_0-rmse:12254.14973\n",
      "[37]\tvalidation_0-rmse:12008.10016\n",
      "[38]\tvalidation_0-rmse:11776.41658\n",
      "[39]\tvalidation_0-rmse:11548.54934\n",
      "[40]\tvalidation_0-rmse:11346.24523\n",
      "[41]\tvalidation_0-rmse:11158.36270\n",
      "[42]\tvalidation_0-rmse:10972.29171\n",
      "[43]\tvalidation_0-rmse:10797.17528\n",
      "[44]\tvalidation_0-rmse:10630.53239\n",
      "[45]\tvalidation_0-rmse:10490.93440\n",
      "[46]\tvalidation_0-rmse:10356.51029\n",
      "[47]\tvalidation_0-rmse:10234.20901\n",
      "[48]\tvalidation_0-rmse:10121.20428\n",
      "[49]\tvalidation_0-rmse:10009.56408\n",
      "[50]\tvalidation_0-rmse:9896.13714\n",
      "[51]\tvalidation_0-rmse:9787.30960\n",
      "[52]\tvalidation_0-rmse:9697.44112\n",
      "[53]\tvalidation_0-rmse:9598.63461\n",
      "[54]\tvalidation_0-rmse:9505.35563\n",
      "[55]\tvalidation_0-rmse:9427.36305\n",
      "[56]\tvalidation_0-rmse:9343.83231\n",
      "[57]\tvalidation_0-rmse:9276.10912\n",
      "[58]\tvalidation_0-rmse:9212.32063\n",
      "[59]\tvalidation_0-rmse:9150.42462\n",
      "[60]\tvalidation_0-rmse:9097.97469\n",
      "[61]\tvalidation_0-rmse:9047.37060\n",
      "[62]\tvalidation_0-rmse:8997.05466\n",
      "[63]\tvalidation_0-rmse:8950.33128\n",
      "[64]\tvalidation_0-rmse:8905.04071\n",
      "[65]\tvalidation_0-rmse:8861.52652\n",
      "[66]\tvalidation_0-rmse:8821.15844\n",
      "[67]\tvalidation_0-rmse:8782.16458\n",
      "[68]\tvalidation_0-rmse:8746.81251\n",
      "[69]\tvalidation_0-rmse:8712.37698\n",
      "[70]\tvalidation_0-rmse:8680.02486\n",
      "[71]\tvalidation_0-rmse:8649.10993\n",
      "[72]\tvalidation_0-rmse:8620.26398\n",
      "[73]\tvalidation_0-rmse:8593.04563\n",
      "[74]\tvalidation_0-rmse:8566.80888\n",
      "[75]\tvalidation_0-rmse:8543.11090\n",
      "[76]\tvalidation_0-rmse:8519.15046\n",
      "[77]\tvalidation_0-rmse:8497.13812\n",
      "[78]\tvalidation_0-rmse:8476.16876\n",
      "[79]\tvalidation_0-rmse:8456.45823\n",
      "[80]\tvalidation_0-rmse:8438.03057\n",
      "[81]\tvalidation_0-rmse:8421.33814\n",
      "[82]\tvalidation_0-rmse:8400.99344\n",
      "[83]\tvalidation_0-rmse:8381.10819\n",
      "[84]\tvalidation_0-rmse:8362.29314\n",
      "[85]\tvalidation_0-rmse:8344.49355\n",
      "[86]\tvalidation_0-rmse:8327.63646\n",
      "[87]\tvalidation_0-rmse:8311.37675\n",
      "[88]\tvalidation_0-rmse:8296.16712\n",
      "[89]\tvalidation_0-rmse:8282.11999\n",
      "[90]\tvalidation_0-rmse:8268.24656\n",
      "[91]\tvalidation_0-rmse:8255.16691\n",
      "[92]\tvalidation_0-rmse:8242.85320\n",
      "[93]\tvalidation_0-rmse:8230.90580\n",
      "[94]\tvalidation_0-rmse:8219.58917\n",
      "[95]\tvalidation_0-rmse:8208.94431\n",
      "[96]\tvalidation_0-rmse:8198.89177\n",
      "[97]\tvalidation_0-rmse:8188.73907\n",
      "[98]\tvalidation_0-rmse:8179.78291\n",
      "[99]\tvalidation_0-rmse:8171.09922\n",
      "[100]\tvalidation_0-rmse:8165.04903\n",
      "[101]\tvalidation_0-rmse:8157.33310\n",
      "[102]\tvalidation_0-rmse:8151.40123\n",
      "[103]\tvalidation_0-rmse:8145.83577\n",
      "[104]\tvalidation_0-rmse:8139.34688\n",
      "[105]\tvalidation_0-rmse:8134.10413\n",
      "[106]\tvalidation_0-rmse:8129.60241\n",
      "[107]\tvalidation_0-rmse:8124.75316\n",
      "[108]\tvalidation_0-rmse:8120.33328\n",
      "[109]\tvalidation_0-rmse:8116.23433\n",
      "[110]\tvalidation_0-rmse:8112.31459\n",
      "[111]\tvalidation_0-rmse:8108.48743\n",
      "[112]\tvalidation_0-rmse:8104.96458\n",
      "[113]\tvalidation_0-rmse:8101.55731\n",
      "[114]\tvalidation_0-rmse:8098.49297\n",
      "[115]\tvalidation_0-rmse:8095.37408\n",
      "[116]\tvalidation_0-rmse:8092.41524\n",
      "[117]\tvalidation_0-rmse:8089.78597\n",
      "[118]\tvalidation_0-rmse:8087.13071\n",
      "[119]\tvalidation_0-rmse:8084.71035\n",
      "[120]\tvalidation_0-rmse:8082.31571\n",
      "[121]\tvalidation_0-rmse:8080.16031\n",
      "[122]\tvalidation_0-rmse:8078.27362\n",
      "[123]\tvalidation_0-rmse:8076.40166\n",
      "[124]\tvalidation_0-rmse:8074.61743\n",
      "[125]\tvalidation_0-rmse:8072.98518\n",
      "[126]\tvalidation_0-rmse:8071.42794\n",
      "[127]\tvalidation_0-rmse:8069.84282\n",
      "[128]\tvalidation_0-rmse:8068.27223\n",
      "[129]\tvalidation_0-rmse:8066.88822\n",
      "[130]\tvalidation_0-rmse:8065.54961\n",
      "[131]\tvalidation_0-rmse:8064.29547\n",
      "[132]\tvalidation_0-rmse:8062.99214\n",
      "[133]\tvalidation_0-rmse:8061.55385\n",
      "[134]\tvalidation_0-rmse:8060.37508\n",
      "[135]\tvalidation_0-rmse:8059.42157\n",
      "[136]\tvalidation_0-rmse:8058.27495\n",
      "[137]\tvalidation_0-rmse:8057.36813\n",
      "[138]\tvalidation_0-rmse:8056.55308\n",
      "[139]\tvalidation_0-rmse:8055.73002\n",
      "[140]\tvalidation_0-rmse:8055.12288\n",
      "[141]\tvalidation_0-rmse:8054.38447\n",
      "[142]\tvalidation_0-rmse:8053.59046\n",
      "[143]\tvalidation_0-rmse:8053.28426\n",
      "[144]\tvalidation_0-rmse:8052.69878\n",
      "[145]\tvalidation_0-rmse:8052.05725\n",
      "[146]\tvalidation_0-rmse:8051.45631\n",
      "[147]\tvalidation_0-rmse:8050.94184\n",
      "[148]\tvalidation_0-rmse:8050.52157\n",
      "[149]\tvalidation_0-rmse:8049.92925\n",
      "[150]\tvalidation_0-rmse:8049.46509\n",
      "[151]\tvalidation_0-rmse:8049.07819\n",
      "[152]\tvalidation_0-rmse:8048.71756\n",
      "[153]\tvalidation_0-rmse:8048.33850\n",
      "[154]\tvalidation_0-rmse:8048.00818\n",
      "[155]\tvalidation_0-rmse:8047.64421\n",
      "[156]\tvalidation_0-rmse:8047.57745\n",
      "[157]\tvalidation_0-rmse:8046.97197\n",
      "[158]\tvalidation_0-rmse:8047.13206\n",
      "[159]\tvalidation_0-rmse:8046.66604\n",
      "[160]\tvalidation_0-rmse:8046.60352\n",
      "[161]\tvalidation_0-rmse:8046.31113\n",
      "[162]\tvalidation_0-rmse:8046.31021\n",
      "[163]\tvalidation_0-rmse:8046.05678\n",
      "[164]\tvalidation_0-rmse:8045.89112\n",
      "[165]\tvalidation_0-rmse:8045.92468\n",
      "[166]\tvalidation_0-rmse:8045.99745\n",
      "[167]\tvalidation_0-rmse:8046.19028\n",
      "[168]\tvalidation_0-rmse:8046.01854\n",
      "[169]\tvalidation_0-rmse:8045.80631\n",
      "[170]\tvalidation_0-rmse:8045.83148\n",
      "[171]\tvalidation_0-rmse:8045.73121\n",
      "[172]\tvalidation_0-rmse:8045.73991\n",
      "[173]\tvalidation_0-rmse:8045.74384\n",
      "[174]\tvalidation_0-rmse:8045.57480\n",
      "[175]\tvalidation_0-rmse:8045.59995\n",
      "[176]\tvalidation_0-rmse:8045.62057\n",
      "[177]\tvalidation_0-rmse:8045.61372\n",
      "[178]\tvalidation_0-rmse:8045.61076\n",
      "[179]\tvalidation_0-rmse:8045.62109\n",
      "[180]\tvalidation_0-rmse:8045.58508\n",
      "[181]\tvalidation_0-rmse:8045.95061\n",
      "[182]\tvalidation_0-rmse:8045.96641\n",
      "[183]\tvalidation_0-rmse:8046.22896\n",
      "[184]\tvalidation_0-rmse:8046.27229\n",
      "[185]\tvalidation_0-rmse:8046.22531\n",
      "[186]\tvalidation_0-rmse:8046.24600\n",
      "[187]\tvalidation_0-rmse:8046.21207\n",
      "[188]\tvalidation_0-rmse:8046.19137\n",
      "[189]\tvalidation_0-rmse:8046.17560\n",
      "[190]\tvalidation_0-rmse:8046.23247\n",
      "[191]\tvalidation_0-rmse:8046.20097\n",
      "[192]\tvalidation_0-rmse:8046.19146\n",
      "[193]\tvalidation_0-rmse:8046.15927\n",
      "[194]\tvalidation_0-rmse:8046.17599\n",
      "[195]\tvalidation_0-rmse:8046.18623\n",
      "[196]\tvalidation_0-rmse:8046.15380\n",
      "[197]\tvalidation_0-rmse:8046.27121\n",
      "[198]\tvalidation_0-rmse:8046.30952\n",
      "[199]\tvalidation_0-rmse:8046.30348\n",
      "[200]\tvalidation_0-rmse:8046.32530\n",
      "[201]\tvalidation_0-rmse:8046.27250\n",
      "[202]\tvalidation_0-rmse:8046.35165\n",
      "[203]\tvalidation_0-rmse:8046.32403\n",
      "[204]\tvalidation_0-rmse:8046.31951\n",
      "[205]\tvalidation_0-rmse:8046.41039\n",
      "[206]\tvalidation_0-rmse:8046.44516\n",
      "[207]\tvalidation_0-rmse:8046.52084\n",
      "[208]\tvalidation_0-rmse:8046.52576\n",
      "[209]\tvalidation_0-rmse:8046.50404\n",
      "[210]\tvalidation_0-rmse:8046.49098\n",
      "[211]\tvalidation_0-rmse:8046.47036\n",
      "[212]\tvalidation_0-rmse:8046.47644\n",
      "[213]\tvalidation_0-rmse:8046.46477\n",
      "[214]\tvalidation_0-rmse:8046.46673\n",
      "[215]\tvalidation_0-rmse:8046.44846\n",
      "[216]\tvalidation_0-rmse:8046.43630\n",
      "[217]\tvalidation_0-rmse:8046.43150\n",
      "[218]\tvalidation_0-rmse:8046.43635\n",
      "[219]\tvalidation_0-rmse:8046.41221\n",
      "[220]\tvalidation_0-rmse:8046.42650\n",
      "[221]\tvalidation_0-rmse:8046.40465\n",
      "[222]\tvalidation_0-rmse:8046.35125\n",
      "[223]\tvalidation_0-rmse:8046.35138\n",
      "[224]\tvalidation_0-rmse:8046.32682\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=20000; total time=  37.6s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=20000; total time=  35.7s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=20000; total time=  42.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=8, model__n_estimators=20000; total time= 1.0min\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=8, model__n_estimators=20000; total time=  48.9s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=8, model__n_estimators=20000; total time= 1.0min\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=10, model__n_estimators=20000; total time= 1.2min\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=10, model__n_estimators=20000; total time= 1.0min\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=10, model__n_estimators=20000; total time= 1.1min\n",
      "Best parameters: {'model__learning_rate': 0.1, 'model__max_depth': 6, 'model__n_estimators': 20000}\n",
      "Best score: -45476706.77803424\n",
      "MSE: 6562621530.615354\n"
     ]
    }
   ],
   "source": [
    "#, early_stopping_rounds=50\n",
    "model = xgb.XGBRegressor(n_estimators=20000, learning_rate=0.1, max_depth=8,enable_categorical=True)\n",
    "# Define the simplified pipeline with only the model\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "for col in categorical_cols:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_val[col] = X_val[col].astype('category')\n",
    "\n",
    "my_pipeline.fit(\n",
    "    X_train_preprocessed, y_train,\n",
    "    model__eval_set=[(X_val_preprocessed, y_val)],  # Pass preprocessed validation data\n",
    "    model__early_stopping_rounds=50,               # Set early stopping rounds\n",
    "    model__verbose=True                            # Enable verbose output for training progress\n",
    ")\n",
    "##############################\n",
    "#let XGBRegressor handle mising value and one hot\n",
    "###############################\n",
    "#my_pipeline.fit(\n",
    "#    X_train, y_train,\n",
    "#    model__eval_set=[(X_val, y_val)],  # Pass preprocessed validation data\n",
    "#    model__early_stopping_rounds=50,               # Set early stopping rounds\n",
    "#    model__verbose=True                            # Enable verbose output for training progress\n",
    "#)\n",
    "\n",
    "\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'model__n_estimators': [20000],\n",
    "    'model__max_depth': [6,8,10],\n",
    "    'model__learning_rate': [0.1]#0.01,0.05,0.1,0.2],\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(my_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)\n",
    "\n",
    "# Fit the pipeline with GridSearchCV\n",
    "grid_search.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "#Best parameters: {'model__max_depth': 8, 'model__n_estimators': 20000}\n",
    "#Best score: -35096476.62818256\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "predictions_xgb = my_pipeline.predict(X_test_preprocessed)\n",
    "\n",
    "score = mean_squared_error(y_aligned, predictions_xgb)\n",
    "print('MSE:', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73fb399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T09:10:16.727597Z",
     "iopub.status.busy": "2024-12-31T09:10:16.727209Z",
     "iopub.status.idle": "2024-12-31T09:10:16.769101Z",
     "shell.execute_reply": "2024-12-31T09:10:16.766272Z"
    },
    "papermill": {
     "duration": 0.062994,
     "end_time": "2024-12-31T09:10:16.772276",
     "exception": false,
     "start_time": "2024-12-31T09:10:16.709282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso Mean Squared Error: 6563472598.4916115\n"
     ]
    }
   ],
   "source": [
    "# Train LASSO regression\n",
    "lasso = Lasso(alpha=0.1)  # Alpha is the regularization strength\n",
    "lasso.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = lasso.predict(X_test_preprocessed)\n",
    "mse = mean_squared_error(y_aligned, y_pred)\n",
    "print(\"lasso Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bfb594d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T09:10:16.877342Z",
     "iopub.status.busy": "2024-12-31T09:10:16.876712Z",
     "iopub.status.idle": "2024-12-31T09:10:16.980359Z",
     "shell.execute_reply": "2024-12-31T09:10:16.978866Z"
    },
    "papermill": {
     "duration": 0.159431,
     "end_time": "2024-12-31T09:10:16.982734",
     "exception": false,
     "start_time": "2024-12-31T09:10:16.823303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 2)\n",
      "Your submission was successfully saved!\n",
      "     Id  SalePrice\n",
      "0  1461     165182\n",
      "1  1462     164862\n",
      "2  1463     164933\n",
      "3  1464     164964\n",
      "4  1465     164943\n"
     ]
    }
   ],
   "source": [
    "output = pd.DataFrame({'Id': test_data.Id, 'SalePrice': predictions_xgb})\n",
    "output.SalePrice = output.SalePrice.astype(int)\n",
    "print(output.shape)\n",
    "\n",
    "output.to_csv('submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")\n",
    "\n",
    "print(output.head())\n",
    "\n",
    "X_train_preprocessed.columns = preprocessor.get_feature_names_out()\n",
    "X_train_preprocessed.to_csv('transformed_data_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 868283,
     "sourceId": 5407,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 535.053022,
   "end_time": "2024-12-31T09:10:17.925376",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-31T09:01:22.872354",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
